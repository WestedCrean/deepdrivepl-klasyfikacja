{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPLKO Moduł 4 - praca domowa - Quickdraw 10 class - regularyzacja\n",
    "\n",
    "Twoim zadaniem w tym module będzie przygotowanie własnego modelu sieci neuronowej korzystając z regularyzacji.\n",
    "\n",
    "Lista rzeczy które musi spełnić Twój model:\n",
    "- [x] działać na wybranych przez Ciebie 10 klasach (bazuj na kodzie z modułu 3)\n",
    "- [ ] liczba parametrów pomiędzy 100'000 a 200'000\n",
    "- [ ] wykorzystane przynajmniej 2 sposoby walki z regularyzacją\n",
    "- [ ] mieć wykonane co najmniej 4 zmiany w celu poprawy wyniku; zachowaj wszystkie iteracje (modyfikując model możesz dodać opcje w funkji, bądź skopiować klasę/funkcję, tak by było widać kolejne architektury)\n",
    "- [ ] opisz co chcesz sprawdzić w kolejnych eksperymentach (np. sprawdzę czy Dropout pomaga i z jaką wartością drop ratio najbardziej)\n",
    "- [ ] uzyskiwać lepsze `validation accuracy` niż w przypadku pierwszego modelu z poprzedniego modułu (im więcej punktów procentowych różnicy tym lepiej)\n",
    "\n",
    "Zwizualizuj proszę:\n",
    "- [ ] historie treningów (wystarczy Val acc, ale train acc czy lossy też mogą być)\n",
    "- [ ] zależność: liczba parametrów - val acc\n",
    "\n",
    "Możesz (czyli opcjonalne rzeczy):\n",
    "- pracować na zmniejszonym zbiorze, by dobrać wartość parametrów\n",
    "- np. zastosować dropout, pooling i early stopping\n",
    "- zastosować TF2 - Keras / PyTorcha czy PL (Pytorch Lightning)\n",
    "- dodać LR scheduler do swojego treningu (i sprawdzić czy to poprawiło wynik)\n",
    "- zwizualizować dodatkowo:\n",
    "  - confussion matrix\n",
    "  - błędne przypadki\n",
    "\n",
    "Warto:\n",
    "- zmieniać 1 parametr między eksperymentami (szczególnie trudne gdy się już nabierze wyczucia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane        <class 'numpy.ndarray'>\n",
      "banana          <class 'numpy.ndarray'>\n",
      "cookie          <class 'numpy.ndarray'>\n",
      "diamond         <class 'numpy.ndarray'>\n",
      "dog             <class 'numpy.ndarray'>\n",
      "hot air balloon <class 'numpy.ndarray'>\n",
      "knife           <class 'numpy.ndarray'>\n",
      "parachute       <class 'numpy.ndarray'>\n",
      "scissors        <class 'numpy.ndarray'>\n",
      "wine glass      <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_names = [\"airplane\", \"banana\", \"cookie\", \"diamond\", \"dog\", \"hot air balloon\", \"knife\" ,\"parachute\", \"scissors\", \"wine glass\"]\n",
    "\n",
    "# wczytanie danych\n",
    "\n",
    "data_folder = \"../data/quickdraw/\"\n",
    "\n",
    "for name in class_names:\n",
    "    url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/%s.npy'%name\n",
    "    file_name = data_folder + url.split('/')[-1].split('?')[0]\n",
    "\n",
    "    url = url.replace(' ','%20')\n",
    "\n",
    "    if not os.path.isfile(file_name):\n",
    "        print(url, '==>', file_name)\n",
    "        urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "data = []\n",
    "for name in class_names:\n",
    "    file_name = data_folder + name + '.npy'\n",
    "    data.append(np.load(file_name, fix_imports=True, allow_pickle=True))\n",
    "    print('%-15s'%name,type(data[-1]))\n",
    "\n",
    "X = np.concatenate(data).reshape(-1,28,28,1)\n",
    "y = np.concatenate([np.full(d.shape[0], i) for i, d in enumerate(data)])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=12, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchmetrics.functional.classification.accuracy import accuracy\n",
    "\n",
    "class QuickDrawCNN_PL(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        ####################\n",
    "        ### Don't chagne ###\n",
    "        assert type(self.X_train)==torch.Tensor\n",
    "        assert self.X_train.shape==torch.Size([len(X_train), 1, 28, 28])\n",
    "        assert self.X_train.dtype==torch.float32, \"Typ X_train niepoprawny\"\n",
    "\n",
    "        assert type(self.y_train)==torch.Tensor\n",
    "        assert self.y_train.shape==torch.Size([len(X_train)])\n",
    "        assert self.y_train.dtype==torch.int64, \"Typ y_train niepoprawny\"\n",
    "\n",
    "        assert type(self.X_val)==torch.Tensor\n",
    "        assert self.X_val.shape==torch.Size([len(X_val), 1, 28, 28])\n",
    "        assert self.X_val.dtype==torch.float32, \"Typ X_val niepoprawny\"\n",
    "\n",
    "        assert type(self.y_val)==torch.Tensor\n",
    "        assert self.y_val.shape==torch.Size([len(y_val)])\n",
    "        assert self.y_val.dtype==torch.int64, \"Typ y_val niepoprawny\"\n",
    "        ### Don't chagne ###\n",
    "        ####################\n",
    "\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "        ####################\n",
    "        ### Don't chagne ###\n",
    "        assert any(['Conv2d' in str(_) for _ in self.model]), \"Zastosuj przynajmniej jedną warstwę Conv2d\"\n",
    "        assert len([_ for _ in self.model if 'Conv2d' in str(_)])==len([_ for _ in self.model if 'ReLU' in str(_)]), \"Po każdej warstwie Conv2d zastosuj funkcję aktywacji ReLU\"\n",
    "        ### Don't chagne ###\n",
    "        ####################\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuickDrawCNN_PL(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Flatten(start_dim=1, end_dim=-1)\n",
      "    (10): Linear(in_features=256, out_features=32, bias=True)\n",
      "    (11): Linear(in_features=32, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 175082\n"
     ]
    }
   ],
   "source": [
    "# check model summary \n",
    "model = QuickDrawCNN_PL(X_train,y_train,X_val,y_val, batch_size=32)\n",
    "print(model)\n",
    "\n",
    "# check number of parameters\n",
    "num_of_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of parameters:', num_of_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert num_of_params > 100_000, \"Za mało parametrów\"\n",
    "assert num_of_params < 200_000, \"Za dużo parametrów\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\fabric\\connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\modul_3_base\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 175 K \n",
      "-------------------------------------\n",
      "175 K     Trainable params\n",
      "0         Non-trainable params\n",
      "175 K     Total params\n",
      "0.700     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bcc4885ce5459289a1b90f2909352e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. \n",
    "# To properly utilize them, you should set \n",
    "# `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = QuickDrawCNN_PL(X_train,y_train,X_val,y_val, BATCH_SIZE)\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_base\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 19528), started 0:24:25 ago. (Use '!kill 19528' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aead5df2764e0154\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aead5df2764e0154\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pierwsza zmiana\n",
    "\n",
    "Dodaję LR scheduler (OneCycleLR), żeby skrócić czas treningu i przyspieszyć kolejne iteracje zmian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\fabric\\connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\modul_3_OneCycleLR\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 175 K \n",
      "-------------------------------------\n",
      "175 K     Trainable params\n",
      "0         Non-trainable params\n",
      "175 K     Total params\n",
      "0.700     Total estimated model params size (MB)\n",
      "C:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loggers\\tensorboard.py:192: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aca1864b3ce4c699b3d7e441601f30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class QuickDrawCNN_PL_LR_Scheduler(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True, shuffle=True)\n",
    "\n",
    "model = QuickDrawCNN_PL_LR_Scheduler(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    limit_train_batches=0.6,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 19528), started 0:35:46 ago. (Use '!kill 19528' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6be038d0cb8aedc1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6be038d0cb8aedc1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Druga zmiana\n",
    "\n",
    "Dodaję regularyzację poprzez BatchNormalization w celu poprawienia wyniku po ok 18k kroku, gdy zaczyna się overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3905386240 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mval_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mQuickDrawCNN_PL_BN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m logger \u001b[38;5;241m=\u001b[39m TensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodul_3_OneCycleLR_BatchNorm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     89\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m     90\u001b[0m     precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m     ]\n\u001b[0;32m     96\u001b[0m )\n",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m, in \u001b[0;36mQuickDrawCNN_PL_BN.__init__\u001b[1;34m(self, X_train, y_train, X_val, y_val, batch_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train,y_train,X_val,y_val, batch_size):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(X_val)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(y_train)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3905386240 bytes."
     ]
    }
   ],
   "source": [
    "class QuickDrawCNN_PL_BN(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True, shuffle=True)\n",
    "\n",
    "model = QuickDrawCNN_PL_BN(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR_BatchNorm\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    limit_train_batches=0.6,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trzecia zmiana\n",
    "\n",
    "Porównujemy z regularyzacją poprzez Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 175 K \n",
      "-------------------------------------\n",
      "175 K     Trainable params\n",
      "0         Non-trainable params\n",
      "175 K     Total params\n",
      "0.703     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46c5da621cf4d559e876d73a70c81c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000229E001A290>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x000002292C8060E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\_weakrefset.py\", line 39, in _remove\n",
      "    def _remove(item, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class QuickDrawCNN_PL_Dropout(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True)\n",
    "\n",
    "model = QuickDrawCNN_PL_Dropout(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR_BatchNorm\", log_graph=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czwarta zmiana pt1 - GlobalAvgPool\n",
    "\n",
    "Zmiana warstwy MaxPool na GlobalAvgPool - za pomocą nn.AdaptiveAvgPool2d((1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=(-2, -1))\n",
    "    \n",
    "class QuickDrawCNN_PL_GlobalAvgPool(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True)\n",
    "\n",
    "model = QuickDrawCNN_PL_GlobalAvgPool(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR_BatchNorm_GlobalAvgPool\", log_graph=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czwarta zmiana pt2 - GlobalMaxPool\n",
    "\n",
    "Zamieniamy GlobalAvgPooling na GlobalMaxPooling żeby sprawdzić różnicę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=(-2, -1))\n",
    "    \n",
    "class QuickDrawCNN_PL_GlobalMaxPool(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveMaxPool2d(1),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16, pin_memory=True)\n",
    "\n",
    "model = QuickDrawCNN_PL_GlobalMaxPool(X_train,y_train,X_val,y_val, batch_size=1024)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR_BatchNorm_GlobalAvgPool\", log_graph=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyślij rozwiązanie\n",
    "Możesz skorzystać z jednego z poniższych sposobów:\n",
    "**mailem na specjalny adres** ze strony pracy domowej w panelu programu prześlij jedno z poniższych:\n",
    "- notebooka (jeżeli plik ma mniej niż np. 10MB)\n",
    "- notebooka w zipie\n",
    "- link do Colaba (udostępniony)\n",
    "- link do pliku przez GDrive/Dropboxa/WeTransfer/...\n",
    "- pdfa (poprzez download as pdf)\n",
    "- jako plik w repozytorium na np. GitHubie, by budować swoje portfolio (wtedy uważaj na wielkość pliku, najlepiej kilka MB, Max 25MB)\n",
    "\n",
    "Najlepiej, by w notebooku było widać wyniki uruchomienia komórek, chyba, że przez nie plik będzie mieć 100+MB wtedy najlepiej Colab lub jakieś przemyślenie co poszło nie tak (zbyt dużo dużych zdjęć wyświetlonych w komórkach).\n",
    "\n",
    "## Co otrzymasz?\n",
    "Informację zwrotną z ewentualnymi sugestiami, komentarzami."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

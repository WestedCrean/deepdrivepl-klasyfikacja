{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPLKO Moduł 4 - praca domowa - Quickdraw 10 class - regularyzacja - pytorch\n",
    "\n",
    "Twoim zadaniem w tym module będzie przygotowanie własnego modelu sieci neuronowej korzystając z regularyzacji.\n",
    "\n",
    "Lista rzeczy które musi spełnić Twój model:\n",
    "- [x] działać na wybranych przez Ciebie 10 klasach (bazuj na kodzie z modułu 3)\n",
    "- [x] liczba parametrów pomiędzy 100'000 a 200'000\n",
    "- [x] wykorzystane przynajmniej 2 sposoby walki z regularyzacją\n",
    "- [x] mieć wykonane co najmniej 4 zmiany w celu poprawy wyniku; zachowaj wszystkie iteracje (modyfikując model możesz dodać opcje w funkji, bądź skopiować klasę/funkcję, tak by było widać kolejne architektury)\n",
    "- [x] opisz co chcesz sprawdzić w kolejnych eksperymentach (np. sprawdzę czy Dropout pomaga i z jaką wartością drop ratio najbardziej)\n",
    "- [x] uzyskiwać lepsze `validation accuracy` niż w przypadku pierwszego modelu z poprzedniego modułu (im więcej punktów procentowych różnicy tym lepiej)\n",
    "\n",
    "Zwizualizuj proszę:\n",
    "- [ ] historie treningów (wystarczy Val acc, ale train acc czy lossy też mogą być)\n",
    "- [ ] zależność: liczba parametrów - val acc\n",
    "\n",
    "Możesz (czyli opcjonalne rzeczy):\n",
    "- pracować na zmniejszonym zbiorze, by dobrać wartość parametrów\n",
    "- np. zastosować dropout, pooling i early stopping\n",
    "- zastosować TF2 - Keras / PyTorcha czy PL (Pytorch Lightning)\n",
    "- dodać LR scheduler do swojego treningu (i sprawdzić czy to poprawiło wynik)\n",
    "- zwizualizować dodatkowo:\n",
    "  - confussion matrix\n",
    "  - błędne przypadki\n",
    "\n",
    "Warto:\n",
    "- zmieniać 1 parametr między eksperymentami (szczególnie trudne gdy się już nabierze wyczucia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.6)\n",
      "Requirement already satisfied: wandb in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.15.8)\n",
      "Requirement already satisfied: Jinja2<5.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (3.1.2)\n",
      "Requirement already satisfied: PyYAML<8.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (6.0.1)\n",
      "Requirement already satisfied: arrow<3.0,>=1.2.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.2.3)\n",
      "Requirement already satisfied: backoff<4.0,>=2.2.1 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (2.2.1)\n",
      "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: click<10.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (8.1.6)\n",
      "Requirement already satisfied: croniter<1.5.0,>=1.3.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.4.1)\n",
      "Requirement already satisfied: dateutils<2.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.6.12)\n",
      "Requirement already satisfied: deepdiff<8.0,>=5.7.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (6.3.1)\n",
      "Requirement already satisfied: fastapi<2.0,>=0.92.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.100.1)\n",
      "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (2023.6.0)\n",
      "Requirement already satisfied: inquirer<5.0,>=2.10.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (3.1.3)\n",
      "Requirement already satisfied: lightning-cloud>=0.5.37 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.5.37)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.7.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.24.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (23.1)\n",
      "Requirement already satisfied: psutil<7.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (5.9.5)\n",
      "Requirement already satisfied: pydantic<2.1.0,>=1.7.4 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (2.0.3)\n",
      "Requirement already satisfied: python-multipart<2.0,>=0.0.5 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.0.6)\n",
      "Requirement already satisfied: requests<4.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (2.28.1)\n",
      "Requirement already satisfied: rich<15.0,>=12.3.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (13.5.0)\n",
      "Requirement already satisfied: starlette in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.27.0)\n",
      "Requirement already satisfied: starsessions<2.0,>=1.2.1 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.3.0)\n",
      "Requirement already satisfied: torch<4.0,>=1.11.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (2.0.1+cu117)\n",
      "Requirement already satisfied: torchmetrics<2.0,>=0.7.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.0.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (4.65.0)\n",
      "Requirement already satisfied: traitlets<7.0,>=5.3.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (4.7.1)\n",
      "Requirement already satisfied: urllib3<4.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.26.13)\n",
      "Requirement already satisfied: uvicorn<2.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.23.1)\n",
      "Requirement already satisfied: websocket-client<3.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.6.1)\n",
      "Requirement already satisfied: websockets<13.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (11.0.3)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (2.0.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.1.32)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.29.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (65.5.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (4.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click<10.0->lightning) (0.4.6)\n",
      "Requirement already satisfied: pytz in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dateutils<2.0->lightning) (2023.3)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from deepdiff<8.0,>=5.7.0->lightning) (4.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec<2025.0,>=2022.5.0->lightning) (3.8.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: blessed>=1.19.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.20.0)\n",
      "Requirement already satisfied: python-editor>=1.0.4 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.0.4)\n",
      "Requirement already satisfied: readchar>=3.0.6 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from inquirer<5.0,>=2.10.0->lightning) (4.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Jinja2<5.0->lightning) (2.1.2)\n",
      "Requirement already satisfied: pyjwt in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning-cloud>=0.5.37->lightning) (2.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<2.1.0,>=1.7.4->lightning) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.3.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<2.1.0,>=1.7.4->lightning) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<4.0->lightning) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<4.0->lightning) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<4.0->lightning) (2022.12.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich<15.0,>=12.3.0->lightning) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich<15.0,>=12.3.0->lightning) (2.15.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette->lightning) (3.7.1)\n",
      "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<4.0,>=1.11.0->lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<4.0,>=1.11.0->lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<4.0,>=1.11.0->lightning) (3.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn<2.0->lightning) (0.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.4.0->starlette->lightning) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.4.0->starlette->lightning) (1.1.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.6)\n",
      "Requirement already satisfied: jinxed>=1.1.0 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (1.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning) (0.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch<4.0,>=1.11.0->lightning) (1.2.1)\n",
      "Requirement already satisfied: ansicon in c:\\users\\wiktor\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (1.89.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: gratkadlafana. Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "! pip install lightning wandb && wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import urllib\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchmetrics.functional.classification.accuracy import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasa QuickDrawDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytanie danych\n",
    "\n",
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"banana\",\n",
    "    \"cookie\",\n",
    "    \"diamond\",\n",
    "    \"dog\",\n",
    "    \"hot air balloon\",\n",
    "    \"knife\",\n",
    "    \"parachute\",\n",
    "    \"scissors\",\n",
    "    \"wine glass\",\n",
    "]\n",
    "data_folder = \"../data/quickdraw/\"\n",
    "\n",
    "# make sure data_folder exists - pathlib\n",
    "pathlib.Path(data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "class QuickDrawDataset(Dataset):\n",
    "    \"\"\"A Quick, Draw! dataset\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, classes, root_dir, download_data=False, load_data=True, transform=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            classes (list[string]): List of classes to be used.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            download (bool, optional) – If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n",
    "        \"\"\"\n",
    "        self.classes = classes\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        if download_data:\n",
    "            self.download_data()\n",
    "\n",
    "        if load_data:\n",
    "            self.data, self.targets = self._load_data()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, Any]:\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img, target = self.data[idx], int(self.targets[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def download_data(self):\n",
    "        for name in self.classes:\n",
    "            url = (\n",
    "                \"https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/%s.npy\"\n",
    "                % name\n",
    "            )\n",
    "            file_name = self.root_dir + url.split(\"/\")[-1].split(\"?\")[0]\n",
    "\n",
    "            url = url.replace(\" \", \"%20\")\n",
    "\n",
    "            if not os.path.isfile(file_name):\n",
    "                print(url, \"==>\", file_name)\n",
    "                urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "    def _load_data(self):\n",
    "        raw_data = []\n",
    "        for name in self.classes:\n",
    "            file_name = self.root_dir + name + \".npy\"\n",
    "            raw_data.append(np.load(file_name, fix_imports=True, allow_pickle=True))\n",
    "            print(\"%-15s\" % name, type(raw_data[-1]))\n",
    "\n",
    "        reshaped_data = np.concatenate(raw_data).reshape(-1, 28, 28, 1)\n",
    "        reshaped_targets = np.concatenate(\n",
    "            [np.full(d.shape[0], i) for i, d in enumerate(raw_data)]\n",
    "        )\n",
    "\n",
    "        return reshaped_data, reshaped_targets\n",
    "\n",
    "    def _set_data(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def split_train_test(self, test_size=0.2):\n",
    "        \"\"\"Split data into train and test sets using sklearn.model_selectiontrain_test_split function.\"\"\"\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.data,\n",
    "            self.targets,\n",
    "            test_size=test_size,\n",
    "            random_state=12,\n",
    "            stratify=self.targets,\n",
    "        )\n",
    "\n",
    "        train_dataset = QuickDrawDataset(\n",
    "            self.classes,\n",
    "            self.root_dir,\n",
    "            download_data=False,\n",
    "            load_data=False,\n",
    "            transform=self.transform,\n",
    "        )\n",
    "        test_dataset = QuickDrawDataset(\n",
    "            self.classes,\n",
    "            self.root_dir,\n",
    "            download_data=False,\n",
    "            load_data=False,\n",
    "            transform=self.transform,\n",
    "        )\n",
    "\n",
    "        train_dataset._set_data(X_train, y_train)\n",
    "        test_dataset._set_data(X_test, y_test)\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "def get_torch_optimizer(optimizer_name, model_params, lr):\n",
    "    if optimizer_name == \"Adam\":\n",
    "        return torch.optim.Adam(\n",
    "            model_params,\n",
    "            lr=lr,\n",
    "        )\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        return torch.optim.SGD(\n",
    "            model_params,\n",
    "            lr=lr,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer {optimizer_name}\")\n",
    "\n",
    "def get_torch_loss(loss_name):\n",
    "    if loss_name == \"cross_entropy\":\n",
    "        return torch.nn.CrossEntropyLoss()\n",
    "    elif loss_name == \"mse\":\n",
    "        return torch.nn.MSELoss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss {loss_name}\")\n",
    "\n",
    "# from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    loss_samples = []\n",
    "    acc_samples = []\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss_samples.append(loss.item())\n",
    "            acc_samples.append(accuracy(pred, y))\n",
    "     \n",
    "    return np.mean(loss_samples), np.mean(acc_samples)\n",
    "\n",
    "def test(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane        <class 'numpy.ndarray'>\n",
      "banana          <class 'numpy.ndarray'>\n",
      "cookie          <class 'numpy.ndarray'>\n",
      "diamond         <class 'numpy.ndarray'>\n",
      "dog             <class 'numpy.ndarray'>\n",
      "hot air balloon <class 'numpy.ndarray'>\n",
      "knife           <class 'numpy.ndarray'>\n",
      "parachute       <class 'numpy.ndarray'>\n",
      "scissors        <class 'numpy.ndarray'>\n",
      "wine glass      <class 'numpy.ndarray'>\n",
      "train_dataset: 1245340 samples\n",
      "val_dataset: 311335 samples\n"
     ]
    }
   ],
   "source": [
    "all_dataset = QuickDrawDataset(\n",
    "    class_names,\n",
    "    data_folder,\n",
    "    download_data=True,\n",
    "    load_data=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = all_dataset.split_train_test(test_size=0.2)\n",
    "\n",
    "# to save RAM\n",
    "del all_dataset\n",
    "gc.collect()\n",
    "\n",
    "print(f\"train_dataset: {len(train_dataset)} samples\")\n",
    "print(f\"val_dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'device': 'cuda', 'epochs': 10, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "base_config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"device\": (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"cpu\"\n",
    "    ),\n",
    "}\n",
    "pprint.pprint(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowane dataloadery\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=base_config[\"batch_size\"])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=base_config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model bazowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawNetwork(pl.LightningModule):\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 175082\n"
     ]
    }
   ],
   "source": [
    "class QuickDrawNetwork_V1(QuickDrawNetwork):\n",
    "    def __init__(self, dimensions, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels, self.width, self.height = dimensions\n",
    "        self.num_classes = num_classes\n",
    "        self.name = \"Baseline\"\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(self.channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "img_dimensions = (1, 28, 28)\n",
    "model = QuickDrawNetwork_V1(img_dimensions, len(class_names))\n",
    "num_of_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of parameters:', num_of_params)\n",
    "\n",
    "assert num_of_params > 100_000, \"Za mało parametrów\"\n",
    "assert num_of_params < 200_000, \"Za dużo parametrów\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\Baseline\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 175 K \n",
      "-------------------------------------\n",
      "175 K     Trainable params\n",
      "0         Non-trainable params\n",
      "175 K     Total params\n",
      "0.700     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d74869d892451097dd75ca39464930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9ef96c83ca4771b10fd4499949b504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f88447276b4ac38464886ff813e619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7cc47fac7149f5bb73fff67d02ea09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea08233b98ac4360aabfd44ef0d804df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18763298bc0241c08a340102b3815d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa97d0928f942029bcfb4c338130fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d248702f96534fe7a0643039141b94ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abb6be2d0e444a0b074da71e6f30db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca18697f876417da116819bbc40f30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad57dabe6c0c49b1a76504829debd6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e90b9909ef14b2793a790e9c4765106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# trening\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "model = QuickDrawNetwork_V1(img_dimensions, len(class_names))\n",
    "\n",
    "tb_logger = TensorBoardLogger(\"lightning_logs\", name=model.name)\n",
    "wandb_logger = WandbLogger(project=\"deepdrive-modul-4\", name=model.name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=base_config[\"epochs\"], \n",
    "    precision=32,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=[\n",
    "        tb_logger,\n",
    "        wandb_logger\n",
    "    ],\n",
    "    limit_train_batches=0.1, # trenujemy tylko na 10% batchy z podzbioru treningowego\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 zmiana\n",
    "\n",
    "Dodaję LR scheduler (OneCycleLR), żeby skrócić czas treningu i przyspieszyć kolejne iteracje zmian. W tym i w następnych modelach zastosowano również early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawNetwork_V2(QuickDrawNetwork):\n",
    "    def __init__(self, dimensions, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels, self.width, self.height = dimensions\n",
    "        self.num_classes = num_classes\n",
    "        self.name = \"OneCycleLR\"\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(self.channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "model = QuickDrawNetwork_V2(img_dimensions, len(class_names))\n",
    "\n",
    "tb_logger = TensorBoardLogger(\"lightning_logs\", name=model.name)\n",
    "wandb_logger = WandbLogger(project=\"deepdrive-modul-4\", name=model.name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=base_config[\"epochs\"], \n",
    "    precision=32,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=[\n",
    "        tb_logger,\n",
    "        wandb_logger\n",
    "    ],\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ],\n",
    "    limit_train_batches=0.1, # trenujemy tylko na 10% batchy z podzbioru treningowego\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 zmiana\n",
    "\n",
    "Dodaję regularyzację poprzez BatchNormalization w celu poprawienia wyniku po ok 18k kroku, gdy zaczyna się overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawNetwork_V3(QuickDrawNetwork):\n",
    "    def __init__(self, dimensions, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels, self.width, self.height = dimensions\n",
    "        self.num_classes = num_classes\n",
    "        self.name = \"OneCycleLR + BatchNorm\"\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(self.channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "model = QuickDrawNetwork_V3(img_dimensions, len(class_names))\n",
    "\n",
    "tb_logger = TensorBoardLogger(\"lightning_logs\", name=model.name)\n",
    "wandb_logger = WandbLogger(project=\"deepdrive-modul-4\", name=model.name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=base_config[\"epochs\"], \n",
    "    precision=32,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=[\n",
    "        tb_logger,\n",
    "        wandb_logger\n",
    "    ],\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ],\n",
    "    limit_train_batches=0.1, # trenujemy tylko na 10% batchy z podzbioru treningowego\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trzecia zmiana\n",
    "\n",
    "Porównujemy z regularyzacją poprzez Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawNetwork_V4(QuickDrawNetwork):\n",
    "    def __init__(self, dimensions, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels, self.width, self.height = dimensions\n",
    "        self.num_classes = num_classes\n",
    "        self.name = \"OneCycleLR + Dropout\"\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=dropout_ratio[0]),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=dropout_ratio[1]),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=dropout_ratio[1]),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes),  \n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "model = QuickDrawNetwork_V4(img_dimensions, len(class_names))\n",
    "\n",
    "tb_logger = TensorBoardLogger(\"lightning_logs\", name=model.name)\n",
    "wandb_logger = WandbLogger(project=\"deepdrive-modul-4\", name=model.name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=base_config[\"epochs\"], \n",
    "    precision=32,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=[\n",
    "        tb_logger,\n",
    "        wandb_logger\n",
    "    ],\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ],\n",
    "    limit_train_batches=0.1, # trenujemy tylko na 10% batchy z podzbioru treningowego\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trzecia zmiana\n",
    "\n",
    "Porównujemy z regularyzacją poprzez Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawNetwork_V5(QuickDrawNetwork):\n",
    "    def __init__(self, dimensions, num_classes, dropout_ratio=[0.2, 0.2]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels, self.width, self.height = dimensions\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        ratio_string = \"_\".join([str(x) for x in ratio])\n",
    "        self.name = f\"OneCycleLR + Dropout ({ratio_string})\"\n",
    "\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=dropout_ratio[0]),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=dropout_ratio[1]),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=dropout_ratio[1]),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes),  \n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "dropout_ratios_same = [[0.2, 0.2], [0.3, 0.3], [0.5, 0.5]]\n",
    "dropout_ratios_different = [ [0.2, 0.5], [0.3, 0.7], [0.5, 0.2] ]\n",
    "\n",
    "\n",
    "for ratio in dropout_ratios_same + dropout_ratios_different:\n",
    "    model = QuickDrawNetwork_V4(img_dimensions, len(class_names))\n",
    "\n",
    "    tb_logger = TensorBoardLogger(\"lightning_logs\", name=model.name)\n",
    "    wandb_logger = WandbLogger(project=\"deepdrive-modul-4\", name=model.name)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=base_config[\"epochs\"], \n",
    "        precision=32,\n",
    "        accelerator=\"gpu\",\n",
    "        logger=[\n",
    "            tb_logger,\n",
    "            wandb_logger\n",
    "        ],\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "        ],\n",
    "        limit_train_batches=0.1, # trenujemy tylko na 10% batchy z podzbioru treningowego\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wniosek: Z wybranych opcji, dropout najlepiej zadziałał usuwając 50% map cech z pierwszej warstwy i 20% z warstw późniejszych.\n",
    "\n",
    "### 4 zmiana - GlobalAvgPool vs GlobalMaxPool\n",
    "\n",
    "Porównujemy nn.AdaptiveMaxPool2d(1) z nn.AdaptiveAvgPool2d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawNetwork_V6(QuickDrawNetwork):\n",
    "    def __init__(self, dimensions, num_classes, global_pooling, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels, self.width, self.height = dimensions\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        batch_norm_part = \" + BatchNorm2d\" if batch_norm else \"\"\n",
    "        self.name = f\"OneCycleLR + Global{pooling_name}Pool2d\" + batch_norm_part\n",
    "\n",
    "        pooling_layer = None\n",
    "        if global_pooling == \"Avg\":\n",
    "            pooling_layer = nn.AdaptiveAvgPool2d\n",
    "        elif global_pooling == \"Max\":\n",
    "            pooling_layer = nn.AdaptiveMaxPoo\n",
    "        else:\n",
    "            pooling_layer = lambda x: x\n",
    "\n",
    "        batch_norm_layer = nn.BatchNorm2d if batch_norm else lambda x: x\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            batch_norm_layer(32),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            batch_norm_layer(64),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            batch_norm_layer(256),\n",
    "            \n",
    "            pooling_layer(1),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes),  \n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "params = {\n",
    "    \"batch_norm\": [True, False],\n",
    "    \"global_pooling\": [\"Avg\", \"Max\"],\n",
    "}\n",
    "\n",
    "for global_poooling, batch_norm in product(params[\"global_pooling\"], params[\"batch_norm\"]):\n",
    "    model = QuickDrawNetwork_V6(img_dimensions, len(class_names), global_pooling, batch_norm)\n",
    "\n",
    "    tb_logger = TensorBoardLogger(\"lightning_logs\", name=model.name)\n",
    "    wandb_logger = WandbLogger(project=\"deepdrive-modul-4\", name=model.name)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=base_config[\"epochs\"], \n",
    "        precision=32,\n",
    "        accelerator=\"gpu\",\n",
    "        logger=[\n",
    "            tb_logger,\n",
    "            wandb_logger\n",
    "        ],\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "        ]\n",
    "        limit_train_batches=0.1, # trenujemy tylko na 10% batchy z podzbioru treningowego\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wniosek: GlobalAvgPool osiąga nieznacznie wyższy wynik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawNetwork_V6(QuickDrawNetwork):\n",
    "    def __init__(self, dimensions, num_classes, global_pooling, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels, self.width, self.height = dimensions\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.name = \"final_model\"\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.5),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "model = QuickDrawNetwork_V7(img_dimensions, len(class_names))\n",
    "\n",
    "tb_logger = TensorBoardLogger(\"lightning_logs\", name=model.name)\n",
    "wandb_logger = WandbLogger(project=\"deepdrive-modul-4\", name=model.name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=base_config[\"epochs\"], \n",
    "    precision=32,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=[\n",
    "        tb_logger,\n",
    "        wandb_logger\n",
    "    ],\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

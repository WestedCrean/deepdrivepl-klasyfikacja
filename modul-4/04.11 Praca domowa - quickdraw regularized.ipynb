{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPLKO Moduł 4 - praca domowa - Quickdraw 10 class - regularyzacja\n",
    "\n",
    "Twoim zadaniem w tym module będzie przygotowanie własnego modelu sieci neuronowej korzystając z regularyzacji.\n",
    "\n",
    "Lista rzeczy które musi spełnić Twój model:\n",
    "- [x] działać na wybranych przez Ciebie 10 klasach (bazuj na kodzie z modułu 3)\n",
    "- [ ] liczba parametrów pomiędzy 100'000 a 200'000\n",
    "- [ ] wykorzystane przynajmniej 2 sposoby walki z regularyzacją\n",
    "- [ ] mieć wykonane co najmniej 4 zmiany w celu poprawy wyniku; zachowaj wszystkie iteracje (modyfikując model możesz dodać opcje w funkji, bądź skopiować klasę/funkcję, tak by było widać kolejne architektury)\n",
    "- [ ] opisz co chcesz sprawdzić w kolejnych eksperymentach (np. sprawdzę czy Dropout pomaga i z jaką wartością drop ratio najbardziej)\n",
    "- [ ] uzyskiwać lepsze `validation accuracy` niż w przypadku pierwszego modelu z poprzedniego modułu (im więcej punktów procentowych różnicy tym lepiej)\n",
    "\n",
    "Zwizualizuj proszę:\n",
    "- [ ] historie treningów (wystarczy Val acc, ale train acc czy lossy też mogą być)\n",
    "- [ ] zależność: liczba parametrów - val acc\n",
    "\n",
    "Możesz (czyli opcjonalne rzeczy):\n",
    "- pracować na zmniejszonym zbiorze, by dobrać wartość parametrów\n",
    "- np. zastosować dropout, pooling i early stopping\n",
    "- zastosować TF2 - Keras / PyTorcha czy PL (Pytorch Lightning)\n",
    "- dodać LR scheduler do swojego treningu (i sprawdzić czy to poprawiło wynik)\n",
    "- zwizualizować dodatkowo:\n",
    "  - confussion matrix\n",
    "  - błędne przypadki\n",
    "\n",
    "Warto:\n",
    "- zmieniać 1 parametr między eksperymentami (szczególnie trudne gdy się już nabierze wyczucia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_names = [\"airplane\", \"banana\", \"cookie\", \"diamond\", \"dog\", \"hot air balloon\", \"knife\" ,\"parachute\", \"scissors\", \"wine glass\"]\n",
    "\n",
    "# wczytanie danych\n",
    "\n",
    "data_folder = \"../data/quickdraw/\"\n",
    "\n",
    "for name in class_names:\n",
    "    url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/%s.npy'%name\n",
    "    file_name = data_folder + url.split('/')[-1].split('?')[0]\n",
    "\n",
    "    url = url.replace(' ','%20')\n",
    "\n",
    "    if not os.path.isfile(file_name):\n",
    "        print(url, '==>', file_name)\n",
    "        urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "data = []\n",
    "for name in class_names:\n",
    "    file_name = data_folder + name + '.npy'\n",
    "    data.append(np.load(file_name, fix_imports=True, allow_pickle=True))\n",
    "    print('%-15s'%name,type(data[-1]))\n",
    "\n",
    "X = np.concatenate(data).reshape(-1,28,28,1)\n",
    "y = np.concatenate([np.full(d.shape[0], i) for i, d in enumerate(data)])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=12, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchmetrics.functional.classification.accuracy import accuracy\n",
    "\n",
    "class QuickDrawCNN_PL(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        ####################\n",
    "        ### Don't chagne ###\n",
    "        assert type(self.X_train)==torch.Tensor\n",
    "        assert self.X_train.shape==torch.Size([len(X_train), 1, 28, 28])\n",
    "        assert self.X_train.dtype==torch.float32, \"Typ X_train niepoprawny\"\n",
    "\n",
    "        assert type(self.y_train)==torch.Tensor\n",
    "        assert self.y_train.shape==torch.Size([len(X_train)])\n",
    "        assert self.y_train.dtype==torch.int64, \"Typ y_train niepoprawny\"\n",
    "\n",
    "        assert type(self.X_val)==torch.Tensor\n",
    "        assert self.X_val.shape==torch.Size([len(X_val), 1, 28, 28])\n",
    "        assert self.X_val.dtype==torch.float32, \"Typ X_val niepoprawny\"\n",
    "\n",
    "        assert type(self.y_val)==torch.Tensor\n",
    "        assert self.y_val.shape==torch.Size([len(y_val)])\n",
    "        assert self.y_val.dtype==torch.int64, \"Typ y_val niepoprawny\"\n",
    "        ### Don't chagne ###\n",
    "        ####################\n",
    "\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "        ####################\n",
    "        ### Don't chagne ###\n",
    "        assert any(['Conv2d' in str(_) for _ in self.model]), \"Zastosuj przynajmniej jedną warstwę Conv2d\"\n",
    "        assert len([_ for _ in self.model if 'Conv2d' in str(_)])==len([_ for _ in self.model if 'ReLU' in str(_)]), \"Po każdej warstwie Conv2d zastosuj funkcję aktywacji ReLU\"\n",
    "        ### Don't chagne ###\n",
    "        ####################\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model summary \n",
    "model = QuickDrawCNN_PL(X_train,y_train,X_val,y_val, batch_size=32)\n",
    "print(model)\n",
    "\n",
    "# check number of parameters\n",
    "num_of_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of parameters:', num_of_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert num_of_params > 100_000, \"Za mało parametrów\"\n",
    "assert num_of_params < 200_000, \"Za dużo parametrów\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. \n",
    "# To properly utilize them, you should set \n",
    "# `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = QuickDrawCNN_PL(X_train,y_train,X_val,y_val, BATCH_SIZE)\n",
    "#logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_base\")\n",
    "logger = WandbLogger()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pierwsza zmiana\n",
    "\n",
    "Dodaję LR scheduler (OneCycleLR), żeby skrócić czas treningu i przyspieszyć kolejne iteracje zmian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawCNN_PL_LR_Scheduler(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "model = QuickDrawCNN_PL_LR_Scheduler(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "#logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR\", log_graph=True)\n",
    "logger = WandbLogger()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Druga zmiana\n",
    "\n",
    "Dodaję regularyzację poprzez BatchNormalization w celu poprawienia wyniku po ok 18k kroku, gdy zaczyna się overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawCNN_PL_BN(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "model = QuickDrawCNN_PL_BN(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "#logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR_BatchNorm\", log_graph=True)\n",
    "logger = WandbLogger()\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trzecia zmiana\n",
    "\n",
    "Porównujemy z regularyzacją poprzez Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawCNN_PL_Dropout(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "model = QuickDrawCNN_PL_Dropout(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "#logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR_BatchNorm\", log_graph=True)\n",
    "logger = WandbLogger()\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czwarta zmiana pt1 - GlobalAvgPool\n",
    "\n",
    "Zmiana warstwy MaxPool na GlobalAvgPool - za pomocą nn.AdaptiveAvgPool2d((1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=(-2, -1))\n",
    "    \n",
    "class QuickDrawCNN_PL_GlobalAvgPool(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "model = QuickDrawCNN_PL_GlobalAvgPool(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "#logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR_BatchNorm_GlobalAvgPool\", log_graph=True)\n",
    "logger = WandbLogger()\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czwarta zmiana pt2 - GlobalMaxPool\n",
    "\n",
    "Zamieniamy GlobalAvgPooling na GlobalMaxPooling żeby sprawdzić różnicę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=(-2, -1))\n",
    "    \n",
    "class QuickDrawCNN_PL_GlobalMaxPool(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveMaxPool2d(1),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=8)\n",
    "\n",
    "model = QuickDrawCNN_PL_GlobalMaxPool(X_train,y_train,X_val,y_val, batch_size=1024)\n",
    "\n",
    "#logger = TensorBoardLogger(\"lightning_logs\", name=\"modul_3_OneCycleLR_BatchNorm_GlobalAvgPool\", log_graph=True)\n",
    "logger = WandbLogger()\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyślij rozwiązanie\n",
    "Możesz skorzystać z jednego z poniższych sposobów:\n",
    "**mailem na specjalny adres** ze strony pracy domowej w panelu programu prześlij jedno z poniższych:\n",
    "- notebooka (jeżeli plik ma mniej niż np. 10MB)\n",
    "- notebooka w zipie\n",
    "- link do Colaba (udostępniony)\n",
    "- link do pliku przez GDrive/Dropboxa/WeTransfer/...\n",
    "- pdfa (poprzez download as pdf)\n",
    "- jako plik w repozytorium na np. GitHubie, by budować swoje portfolio (wtedy uważaj na wielkość pliku, najlepiej kilka MB, Max 25MB)\n",
    "\n",
    "Najlepiej, by w notebooku było widać wyniki uruchomienia komórek, chyba, że przez nie plik będzie mieć 100+MB wtedy najlepiej Colab lub jakieś przemyślenie co poszło nie tak (zbyt dużo dużych zdjęć wyświetlonych w komórkach).\n",
    "\n",
    "## Co otrzymasz?\n",
    "Informację zwrotną z ewentualnymi sugestiami, komentarzami."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

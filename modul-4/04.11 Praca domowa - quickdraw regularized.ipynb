{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPLKO Moduł 4 - praca domowa - Quickdraw 10 class - regularyzacja\n",
    "\n",
    "Twoim zadaniem w tym module będzie przygotowanie własnego modelu sieci neuronowej korzystając z regularyzacji.\n",
    "\n",
    "Lista rzeczy które musi spełnić Twój model:\n",
    "- [x] działać na wybranych przez Ciebie 10 klasach (bazuj na kodzie z modułu 3)\n",
    "- [ ] liczba parametrów pomiędzy 100'000 a 200'000\n",
    "- [ ] wykorzystane przynajmniej 2 sposoby walki z regularyzacją\n",
    "- [ ] mieć wykonane co najmniej 4 zmiany w celu poprawy wyniku; zachowaj wszystkie iteracje (modyfikując model możesz dodać opcje w funkji, bądź skopiować klasę/funkcję, tak by było widać kolejne architektury)\n",
    "- [ ] opisz co chcesz sprawdzić w kolejnych eksperymentach (np. sprawdzę czy Dropout pomaga i z jaką wartością drop ratio najbardziej)\n",
    "- [ ] uzyskiwać lepsze `validation accuracy` niż w przypadku pierwszego modelu z poprzedniego modułu (im więcej punktów procentowych różnicy tym lepiej)\n",
    "\n",
    "Zwizualizuj proszę:\n",
    "- [ ] historie treningów (wystarczy Val acc, ale train acc czy lossy też mogą być)\n",
    "- [ ] zależność: liczba parametrów - val acc\n",
    "\n",
    "Możesz (czyli opcjonalne rzeczy):\n",
    "- pracować na zmniejszonym zbiorze, by dobrać wartość parametrów\n",
    "- np. zastosować dropout, pooling i early stopping\n",
    "- zastosować TF2 - Keras / PyTorcha czy PL (Pytorch Lightning)\n",
    "- dodać LR scheduler do swojego treningu (i sprawdzić czy to poprawiło wynik)\n",
    "- zwizualizować dodatkowo:\n",
    "  - confussion matrix\n",
    "  - błędne przypadki\n",
    "\n",
    "Warto:\n",
    "- zmieniać 1 parametr między eksperymentami (szczególnie trudne gdy się już nabierze wyczucia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_names = [\"airplane\", \"banana\", \"cookie\", \"diamond\", \"dog\", \"hot air balloon\", \"knife\" ,\"parachute\", \"scissors\", \"wine glass\"]\n",
    "\n",
    "# wczytanie danych\n",
    "\n",
    "data_folder = \"../data/quickdraw/\"\n",
    "\n",
    "for name in class_names:\n",
    "    url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/%s.npy'%name\n",
    "    file_name = data_folder + url.split('/')[-1].split('?')[0]\n",
    "\n",
    "    url = url.replace(' ','%20')\n",
    "\n",
    "    if not os.path.isfile(file_name):\n",
    "        print(url, '==>', file_name)\n",
    "        urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "data = []\n",
    "for name in class_names:\n",
    "    file_name = data_folder + name + '.npy'\n",
    "    data.append(np.load(file_name, fix_imports=True, allow_pickle=True))\n",
    "    print('%-15s'%name,type(data[-1]))\n",
    "\n",
    "X = np.concatenate(data).reshape(-1,28,28,1)\n",
    "y = np.concatenate([np.full(d.shape[0], i) for i, d in enumerate(data)])\n",
    "\n",
    "print(f\"Training on {len(X)} samples\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=12, stratify=y)\n",
    "\n",
    "del X\n",
    "del y\n",
    "del data # to save RAM\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "def read_training_data():\n",
    "    all_df = pd.DataFrame(columns=[\"Step\",\"Value\", \"Model\", \"Metric\"])\n",
    "    for metric in [\"train_loss\", \"val_loss\", \"val_acc\"]:\n",
    "        files = glob.glob(f'./training_history/{metric}/modul_3_*.csv')\n",
    "        models = [ file.replace(f'./training_history/{metric}/', '').replace('.csv', '') for file in files]\n",
    "\n",
    "        for model in models:\n",
    "            new_row = pd.read_csv(f'./training_history/{metric}/{model}.csv')\n",
    "            \n",
    "            new_row[\"Model\"] = model\n",
    "            new_row[\"Metric\"] = metric\n",
    "            all_df = pd.concat([all_df, new_row])\n",
    "    return all_df\n",
    "\n",
    "def plot_training_data(all_df, models=None):\n",
    "    if models:\n",
    "        all_df = all_df[all_df[\"Model\"].isin(models)]\n",
    "    \n",
    "    # plot replot where y axis is between 0 and 1\n",
    "    g = sns.relplot(\n",
    "        data=all_df[all_df[\"Value\"] < 1],\n",
    "        x=\"Step\", y=\"Value\", hue=\"Model\", col=\"Metric\", kind=\"line\",\n",
    "        facet_kws=dict(sharey=False),\n",
    "        height=4, aspect=1.5, legend=\"full\"\n",
    "    )\n",
    "\n",
    "\n",
    "#training_log_data = read_training_data()\n",
    "#training_log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "#from pytorch_lightning.loggers import WandbLogger\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchmetrics.functional.classification.accuracy import accuracy\n",
    "\n",
    "class QuickDrawCNN_PL(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        ####################\n",
    "        ### Don't chagne ###\n",
    "        assert type(self.X_train)==torch.Tensor\n",
    "        assert self.X_train.shape==torch.Size([len(X_train), 1, 28, 28])\n",
    "        assert self.X_train.dtype==torch.float32, \"Typ X_train niepoprawny\"\n",
    "\n",
    "        assert type(self.y_train)==torch.Tensor\n",
    "        assert self.y_train.shape==torch.Size([len(X_train)])\n",
    "        assert self.y_train.dtype==torch.int64, \"Typ y_train niepoprawny\"\n",
    "\n",
    "        assert type(self.X_val)==torch.Tensor\n",
    "        assert self.X_val.shape==torch.Size([len(X_val), 1, 28, 28])\n",
    "        assert self.X_val.dtype==torch.float32, \"Typ X_val niepoprawny\"\n",
    "\n",
    "        assert type(self.y_val)==torch.Tensor\n",
    "        assert self.y_val.shape==torch.Size([len(y_val)])\n",
    "        assert self.y_val.dtype==torch.int64, \"Typ y_val niepoprawny\"\n",
    "        ### Don't chagne ###\n",
    "        ####################\n",
    "\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "        ####################\n",
    "        ### Don't chagne ###\n",
    "        assert any(['Conv2d' in str(_) for _ in self.model]), \"Zastosuj przynajmniej jedną warstwę Conv2d\"\n",
    "        assert len([_ for _ in self.model if 'Conv2d' in str(_)])==len([_ for _ in self.model if 'ReLU' in str(_)]), \"Po każdej warstwie Conv2d zastosuj funkcję aktywacji ReLU\"\n",
    "        ### Don't chagne ###\n",
    "        ####################\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "model = QuickDrawCNN_PL(X_train,y_train,X_val,y_val, batch_size=32)\n",
    "num_of_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(model)\n",
    "print('Number of parameters:', num_of_params)\n",
    "\n",
    "assert num_of_params > 100_000, \"Za mało parametrów\"\n",
    "assert num_of_params < 200_000, \"Za dużo parametrów\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. \n",
    "# To properly utilize them, you should set \n",
    "# `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "\n",
    "model = QuickDrawCNN_PL(X_train,y_train,X_val,y_val, BATCH_SIZE)\n",
    "model_name = \"baseline\"\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=model_name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=32, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    limit_train_batches=0.1, # trenujemy tylko na 10% batchy z podzbioru treningowego\n",
    "    callbacks=[\n",
    "        #EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "del logger\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pierwsza zmiana\n",
    "\n",
    "Dodaję LR scheduler (OneCycleLR), żeby skrócić czas treningu i przyspieszyć kolejne iteracje zmian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawCNN_PL_LR_Scheduler(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "model = QuickDrawCNN_PL_LR_Scheduler(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "model_name = \"OneCycleLR\"\n",
    "logger = TensorBoardLogger(\"lightning_logs\", model_name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=32, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    limit_train_batches=0.1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "del logger\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Druga zmiana\n",
    "\n",
    "Dodaję regularyzację poprzez BatchNormalization w celu poprawienia wyniku po ok 18k kroku, gdy zaczyna się overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawCNN_PL_BN(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "model = QuickDrawCNN_PL_BN(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "model_name = \"OneCycleLR_BatchNorm\"\n",
    "logger = TensorBoardLogger(\"lightning_logs\", model_name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=32, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    limit_train_batches=0.1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "del logger\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trzecia zmiana\n",
    "\n",
    "Porównujemy z regularyzacją poprzez Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawCNN_PL_Dropout(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size, dropout_ratio=[0.2, 0.2]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=dropout_ratio[0]),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=dropout_ratio[1]),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=dropout_ratio[1]),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "\n",
    "ratios = [[0.2, 0.2], [0.3, 0.3], [0.5, 0.5]]\n",
    "ratios_different = [ [0.2, 0.5], [0.3, 0.7], [0.5, 0.2] ]\n",
    "\n",
    "\n",
    "for ratio in ratios + ratios_different: \n",
    "    model = QuickDrawCNN_PL_Dropout(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE, dropout_ratio=ratio)\n",
    "\n",
    "    ratio_string = \"_\".join([str(x) for x in ratio])\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f\"modul_3_OneCycleLR_BatchNorm_Dropout_{ratio_string}\", log_graph=True)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10, \n",
    "        precision=32, \n",
    "        accelerator=\"gpu\",\n",
    "        logger=logger,\n",
    "        limit_train_batches=0.1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "    del model\n",
    "    del trainer\n",
    "    del logger\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wniosek: Dropout najlepiej zadziałał z usuwając 50% map cech z pierwszej warstwy i 20% z warstw późniejszych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czwarta zmiana pt1 - GlobalAvgPool\n",
    "\n",
    "Zmiana warstwy MaxPool na GlobalAvgPool - za pomocą nn.AdaptiveAvgPool2d((1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawCNN_PL_GlobalAvgPool(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "model = QuickDrawCNN_PL_GlobalAvgPool(X_train,y_train,X_val,y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "model_name = \"OneCycleLR_BatchNorm_GlobalAvgPool\"\n",
    "logger = TensorBoardLogger(\"lightning_logs\", model_name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=32, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    limit_train_batches=0.1,\n",
    "    callbacks=[\n",
    "        #EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "del logger\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czwarta zmiana pt2 - GlobalMaxPool\n",
    "\n",
    "Zamieniamy GlobalAvgPooling na GlobalMaxPooling żeby sprawdzić różnicę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=(-2, -1))\n",
    "    \n",
    "class QuickDrawCNN_PL_GlobalMaxPool(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveMaxPool2d(1),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "model = QuickDrawCNN_PL_GlobalMaxPool(X_train,y_train,X_val,y_val, batch_size=1024)\n",
    "\n",
    "\n",
    "model_name = \"OneCycleLR_BatchNorm_GlobalMaxPool\"\n",
    "logger = TensorBoardLogger(\"lightning_logs\", model_name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=32, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    limit_train_batches=0.1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "del logger\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wniosek: GlobalAvgPool osiąga nieznacznie wyższy wynik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=(-2, -1))\n",
    "    \n",
    "class QuickDrawCNN_PL_GlobalMaxPool(pl.LightningModule):\n",
    "    def __init__(self, X_train,y_train,X_val,y_val, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X_train = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "        self.X_val = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "        self.y_train = torch.LongTensor(y_train)\n",
    "        self.y_val = torch.LongTensor(y_val)\n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.5),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            \n",
    "\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(32, self.num_classes), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # zmiana tutaj\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=1e-2, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "model = QuickDrawCNN_PL_GlobalMaxPool(X_train,y_train,X_val,y_val, batch_size=1024)\n",
    "\n",
    "model_name = \"final_network\"\n",
    "logger = TensorBoardLogger(\"lightning_logs\", model_name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    precision=32, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_acc\", min_delta=0.01, patience=4, verbose=False, mode=\"max\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "del logger\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalny model osiąga validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyślij rozwiązanie\n",
    "Możesz skorzystać z jednego z poniższych sposobów:\n",
    "**mailem na specjalny adres** ze strony pracy domowej w panelu programu prześlij jedno z poniższych:\n",
    "- notebooka (jeżeli plik ma mniej niż np. 10MB)\n",
    "- notebooka w zipie\n",
    "- link do Colaba (udostępniony)\n",
    "- link do pliku przez GDrive/Dropboxa/WeTransfer/...\n",
    "- pdfa (poprzez download as pdf)\n",
    "- jako plik w repozytorium na np. GitHubie, by budować swoje portfolio (wtedy uważaj na wielkość pliku, najlepiej kilka MB, Max 25MB)\n",
    "\n",
    "Najlepiej, by w notebooku było widać wyniki uruchomienia komórek, chyba, że przez nie plik będzie mieć 100+MB wtedy najlepiej Colab lub jakieś przemyślenie co poszło nie tak (zbyt dużo dużych zdjęć wyświetlonych w komórkach).\n",
    "\n",
    "## Co otrzymasz?\n",
    "Informację zwrotną z ewentualnymi sugestiami, komentarzami."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
